{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoLhACit0FRF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = \"\"\n",
        "#to save openai api ke, which can be used in different scenerios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8P3JR456z9h"
      },
      "source": [
        "Few of the below sections is usefull to handle the scenerio where we need to pass knowledge to gpt-3 as context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x28Sf4Z_0dAz"
      },
      "outputs": [],
      "source": [
        "#this segment can be used to initial a embedding model. Using the sentence transformation, we can achiee a embedding matrix.\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "\n",
        "text ='Abc'\n",
        "model.encode(text).tolist() #exmple how to do encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFRXPX900xkT"
      },
      "outputs": [],
      "source": [
        "def split_text_into_chunks(plain_text, max_chars=2000):\n",
        "    text_chunks = []\n",
        "    current_chunk = \"\"\n",
        "    for line in plain_text.split(\"\\n\"):\n",
        "        if len(current_chunk) + len(line) + 1 <= max_chars:\n",
        "            current_chunk += line + \" \"\n",
        "        else:\n",
        "            text_chunks.append(current_chunk.strip())\n",
        "            current_chunk = line + \" \"\n",
        "    if current_chunk:\n",
        "        text_chunks.append(current_chunk.strip())\n",
        "    return text_chunks\n",
        "\n",
        "#this function is to split the original text in to smaller chunks. As some of the ai models has limitation on size of the data to be passed on at once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WsR_4X_2cYZ"
      },
      "outputs": [],
      "source": [
        "import pinecone\n",
        "pinecone.init(api_key=\"05825101-2760-452e-b534-3508ed2f0a1b\", environment=\"us-east1-gcp\") #initializing pinecone, which is basiclly a ector data base. The api_key nd environment can be found from pinecone dashboard.\n",
        "index = pinecone.Index(\"gpt-test-embedding\") #Initializing with the Index we are using to store the matrix\n",
        "\n",
        "\n",
        "#Funtion to add data after running any embedding algorithm on it\n",
        "def addData(corpusData):\n",
        "    id  = index.describe_index_stats()['total_vector_count']\n",
        "    for i in range(len(corpusData)):\n",
        "        chunk=corpusData[i]\n",
        "        chunkInfo=(str(id+i),\n",
        "                model.encode(chunk).tolist(), #We are using the model to encode the original chunk of text.\n",
        "                {'context': chunk}) #In metadata we are storing the original text here as context. \n",
        "        index.upsert(vectors=[chunkInfo])\n",
        "\n",
        "def find_match(query,k):\n",
        "    query_em = model.encode(query).tolist() #Encoding the text given on a certain search.\n",
        "    result = index.query(query_em, top_k=k, includeMetadata=True) #Using the encoded text to find the best k matches on the ector database.\n",
        "    \n",
        "    return [result['matches'][i]['metadata']['context'] for i in range(k)] #Returning the best results, passing the context, we can also pass other metadata if we want to, i.e title, section etc.\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDysPH886OkE"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "openai.api_key=os.environ['OPENAI_API_KEY']\n",
        "\n",
        "# Using the context achieved from the search based on embedding, we'll pass the result as context. This function will generate a prompt out of that. The prompt can be changed according to the need.\n",
        "def create_prompt(context,query):\n",
        "    header = \"Answer the question as truthfully as possible using the provided context.\\n\"\n",
        "    return header + context + \"\\n\\n\" + query + \"\\n\"\n",
        "\n",
        "# Finally, we are requesting to gpt-3 with the generated prompt using this function.\n",
        "def generate_answer(prompt):\n",
        "    response = openai.Completion.create(\n",
        "    model=\"text-davinci-003\",\n",
        "    prompt=prompt,\n",
        "    temperature=0,\n",
        "    max_tokens=256,\n",
        "    top_p=1,\n",
        "    frequency_penalty=0,\n",
        "    presence_penalty=0,\n",
        "    stop = [' END']\n",
        "    )\n",
        "    return (response.choices[0].text).strip()\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWj3QDCI7C4E"
      },
      "source": [
        "Some of the blocks below is to do indexing from a big data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0T_g61A74Vm"
      },
      "outputs": [],
      "source": [
        "from gpt_index import Document\n",
        "#This section is to preapre an array of docs from the splitted chunks in the format gpt index takes the input.\n",
        "docs = []\n",
        "text = 'dsadas'\n",
        "splitted_text = split_text_into_chunks(text)\n",
        "\n",
        "for x in splitted_text:\n",
        "  y= Document(x)\n",
        "  docs.append(y)\n",
        "print(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1UWzoIy92Gw"
      },
      "outputs": [],
      "source": [
        "from gpt_index import GPTListIndex, GPTSimpleVectorIndex\n",
        "\n",
        "index = GPTListIndex(docs) #Passing the generated array of text.\n",
        "\n",
        "query= 'List all of the articles from the texts.' #Partial Prompt to generate the index\n",
        "\n",
        "response_structure = \"Give the response as an array of object, like, {'title': 'Article 1', 'content': '' }\" #Partial prompt to ensure the formt we expect\n",
        "\n",
        "final_query = query+ '\\n' + response_structure #Putting the prompt all together\n",
        "\n",
        "result = index.query(final_query) #Getting the result from all the text chunks at once! Mke sure to include the openai api ke in the os environment."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
